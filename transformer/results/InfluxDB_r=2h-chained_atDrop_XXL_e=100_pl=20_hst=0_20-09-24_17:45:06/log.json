{"label":"InfluxDB_r=2h-chained_atDrop_XXL_e=100_pl=20_hst=0","mean_squared_error":-1,"runtimes":"Training: 144.21 seconds","length_train_dataset":196091,"length_test_dataset":1,"parameters":"Epochs: 100, hyperparameters: Hyperparamters(gradient_clip_val=6.9953515571, hidden_continuous_size=40, dropout=0.1558743686, attention_head_size=3, learning_rate=0.0039810717, accelerator='auto', hidden_size=70)","prediction":"Prediction(output=Output(prediction=tensor([[[1.0313, 5.3769, 6.5447, 7.0841, 7.4165, 7.6840, 7.9974],\n         [0.9463, 5.2799, 6.5461, 7.1419, 7.4822, 7.8249, 8.1130],\n         [0.8385, 5.1390, 6.4567, 7.1015, 7.4980, 7.8759, 8.1795],\n         [0.6889, 4.9433, 6.3270, 7.0318, 7.5063, 7.9207, 8.2459],\n         [0.4901, 4.6799, 6.1513, 6.9354, 7.5152, 7.9665, 8.3223],\n         [0.2213, 4.3215, 5.9124, 6.8072, 7.5330, 8.0197, 8.4219],\n         [0.0000, 3.8065, 5.5683, 6.6273, 7.5679, 8.0943, 8.5741],\n         [0.0000, 3.0278, 5.0388, 6.3525, 7.6199, 8.2149, 8.8290],\n         [0.0000, 1.9714, 4.2788, 5.9467, 7.6490, 8.3643, 9.1644],\n         [0.0000, 0.9800, 3.4827, 5.4861, 7.5783, 8.4320, 9.3792],\n         [0.0000, 0.3395, 2.8806, 5.0908, 7.4123, 8.3760, 9.3855],\n         [0.0000, 0.0000, 2.4742, 4.7777, 7.2058, 8.2433, 9.2672],\n         [0.0000, 0.0000, 2.1982, 4.5237, 6.9889, 8.0724, 9.0945],\n         [0.0000, 0.0000, 2.0052, 4.3106, 6.7735, 7.8832, 8.9011],\n         [0.0000, 0.0000, 1.8679, 4.1261, 6.5602, 7.6811, 8.6977],\n         [0.0000, 0.0000, 1.7708, 3.9620, 6.3454, 7.4648, 8.4849],\n         [0.0000, 0.0000, 1.7058, 3.8143, 6.1263, 7.2319, 8.2608],\n         [0.0000, 0.0000, 1.6705, 3.6832, 5.9031, 6.9823, 8.0256],\n         [0.0000, 0.0000, 1.6651, 3.5725, 5.6803, 6.7204, 7.7827],\n         [0.0000, 0.0000, 1.6905, 3.4865, 5.4648, 6.4541, 7.5384]]],\n       device='cuda:0'), encoder_attention=tensor([[[[2.9625e-02, 7.3761e-02, 1.1076e-01,  ..., 1.4304e-04,\n           9.8785e-05, 1.0550e-04],\n          [1.7680e-03, 1.8863e-03, 2.0514e-03,  ..., 6.6467e-02,\n           5.7126e-02, 6.1365e-02],\n          [3.5088e-03, 3.0740e-02, 1.0277e-01,  ..., 1.1926e-03,\n           1.0762e-03, 1.1827e-03],\n          [2.9390e-02, 5.4838e-02, 7.8873e-02,  ..., 6.0616e-04,\n           4.3332e-04, 4.7062e-04]],\n\n         [[3.0201e-02, 7.3773e-02, 1.0987e-01,  ..., 1.6471e-04,\n           1.1467e-04, 1.2229e-04],\n          [3.1503e-04, 3.3753e-04, 3.6801e-04,  ..., 1.1332e-02,\n           9.7401e-03, 1.0463e-02],\n          [3.3104e-03, 2.9455e-02, 9.9509e-02,  ..., 1.1480e-03,\n           1.0337e-03, 1.1379e-03],\n          [2.9680e-02, 5.4478e-02, 7.7942e-02,  ..., 6.5471e-04,\n           4.6711e-04, 5.0770e-04]],\n\n         [[3.0522e-02, 7.3646e-02, 1.0923e-01,  ..., 1.9334e-04,\n           1.3594e-04, 1.4473e-04],\n          [1.5878e-04, 1.6897e-04, 1.8346e-04,  ..., 5.6542e-03,\n           4.8579e-03, 5.2201e-03],\n          [3.0134e-03, 2.7856e-02, 9.6069e-02,  ..., 1.0207e-03,\n           9.1816e-04, 1.0120e-03],\n          [3.0584e-02, 5.4434e-02, 7.6771e-02,  ..., 7.9505e-04,\n           5.6786e-04, 6.1718e-04]],\n\n         ...,\n\n         [[3.1141e-02, 5.9717e-02, 8.2599e-02,  ..., 2.6426e-03,\n           2.1957e-03, 2.2725e-03],\n          [1.4251e-05, 1.3497e-05, 1.3406e-05,  ..., 2.7786e-04,\n           2.4373e-04, 2.6017e-04],\n          [2.4528e-04, 2.3479e-03, 8.0260e-03,  ..., 4.2731e-05,\n           4.0179e-05, 4.3261e-05],\n          [2.6345e-02, 3.0428e-02, 3.3997e-02,  ..., 9.0888e-03,\n           6.9328e-03, 7.4227e-03]],\n\n         [[3.1263e-02, 6.0518e-02, 8.4024e-02,  ..., 2.4860e-03,\n           2.0571e-03, 2.1306e-03],\n          [1.4122e-05, 1.3419e-05, 1.3325e-05,  ..., 2.9018e-04,\n           2.5471e-04, 2.7187e-04],\n          [2.6531e-04, 2.5384e-03, 8.6579e-03,  ..., 4.3370e-05,\n           4.0925e-05, 4.3975e-05],\n          [2.7455e-02, 3.2379e-02, 3.6486e-02,  ..., 8.5761e-03,\n           6.5431e-03, 7.0033e-03]],\n\n         [[3.1380e-02, 6.1355e-02, 8.5522e-02,  ..., 2.3208e-03,\n           1.9115e-03, 1.9814e-03],\n          [1.4222e-05, 1.3559e-05, 1.3462e-05,  ..., 3.0870e-04,\n           2.7115e-04, 2.8941e-04],\n          [2.9202e-04, 2.7861e-03, 9.4698e-03,  ..., 4.4928e-05,\n           4.2545e-05, 4.5623e-05],\n          [2.8646e-02, 3.4566e-02, 3.9323e-02,  ..., 8.0121e-03,\n           6.1126e-03, 6.5409e-03]]]], device='cuda:0'), decoder_attention=tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00]],\n\n         [[2.4370e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [8.2948e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [3.4765e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [1.5174e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00]],\n\n         [[2.9237e-05, 2.3154e-05, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [4.2727e-01, 4.8770e-01, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [3.2831e-02, 3.6407e-02, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [1.7178e-04, 1.7164e-04, 0.0000e+00,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00]],\n\n         ...,\n\n         [[6.0218e-04, 5.2861e-04, 4.5430e-04,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [1.4330e-02, 1.6057e-02, 1.8856e-02,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [1.0044e-03, 1.0791e-03, 1.3835e-03,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00],\n          [9.5862e-04, 9.8453e-04, 8.8492e-04,  ..., 0.0000e+00,\n           0.0000e+00, 0.0000e+00]],\n\n         [[5.5952e-04, 4.8979e-04, 4.1960e-04,  ..., 1.9641e-04,\n           0.0000e+00, 0.0000e+00],\n          [1.4004e-02, 1.5698e-02, 1.8411e-02,  ..., 4.8001e-02,\n           0.0000e+00, 0.0000e+00],\n          [9.7581e-04, 1.0460e-03, 1.3344e-03,  ..., 5.6149e-02,\n           0.0000e+00, 0.0000e+00],\n          [9.3845e-04, 9.6067e-04, 8.6211e-04,  ..., 2.0004e-04,\n           0.0000e+00, 0.0000e+00]],\n\n         [[5.1455e-04, 4.4904e-04, 3.8333e-04,  ..., 1.7495e-04,\n           1.9769e-04, 0.0000e+00],\n          [1.3880e-02, 1.5567e-02, 1.8229e-02,  ..., 4.5356e-02,\n           3.8142e-02, 0.0000e+00],\n          [9.6498e-04, 1.0320e-03, 1.3100e-03,  ..., 5.3110e-02,\n           4.1870e-02, 0.0000e+00],\n          [9.1261e-04, 9.3099e-04, 8.3402e-04,  ..., 1.9352e-04,\n           1.9655e-04, 0.0000e+00]]]], device='cuda:0'), static_variables=tensor([[[0.7451, 0.1265, 0.1285]]], device='cuda:0'), encoder_variables=tensor([[[[0.0496, 0.2490, 0.7013]],\n\n         [[0.0496, 0.2506, 0.6998]],\n\n         [[0.0496, 0.2510, 0.6994]],\n\n         [[0.0496, 0.2488, 0.7016]],\n\n         [[0.0496, 0.2511, 0.6993]],\n\n         [[0.0497, 0.2478, 0.7025]],\n\n         [[0.0496, 0.2508, 0.6996]],\n\n         [[0.0496, 0.2509, 0.6995]],\n\n         [[0.0496, 0.2494, 0.7009]],\n\n         [[0.0496, 0.2497, 0.7007]],\n\n         [[0.0496, 0.2497, 0.7007]],\n\n         [[0.0498, 0.2367, 0.7135]],\n\n         [[0.0500, 0.2241, 0.7259]],\n\n         [[0.0501, 0.2163, 0.7336]],\n\n         [[0.0502, 0.2136, 0.7362]],\n\n         [[0.0501, 0.2184, 0.7315]],\n\n         [[0.0504, 0.2038, 0.7458]],\n\n         [[0.0502, 0.2112, 0.7385]],\n\n         [[0.0505, 0.2037, 0.7459]],\n\n         [[0.0502, 0.2145, 0.7353]],\n\n         [[0.0502, 0.2114, 0.7383]],\n\n         [[0.0502, 0.2114, 0.7383]],\n\n         [[0.0503, 0.2103, 0.7394]],\n\n         [[0.0505, 0.2018, 0.7477]],\n\n         [[0.0502, 0.2114, 0.7384]],\n\n         [[0.0505, 0.2033, 0.7462]],\n\n         [[0.0507, 0.1977, 0.7516]],\n\n         [[0.0504, 0.2039, 0.7457]],\n\n         [[0.0505, 0.2008, 0.7487]],\n\n         [[0.0506, 0.1997, 0.7497]],\n\n         [[0.0507, 0.1964, 0.7529]],\n\n         [[0.0508, 0.1931, 0.7560]],\n\n         [[0.0507, 0.1967, 0.7527]],\n\n         [[0.0505, 0.2018, 0.7477]],\n\n         [[0.0507, 0.1950, 0.7542]],\n\n         [[0.0510, 0.1895, 0.7595]],\n\n         [[0.0509, 0.1907, 0.7584]],\n\n         [[0.0517, 0.1758, 0.7726]],\n\n         [[0.0516, 0.1763, 0.7721]],\n\n         [[0.0519, 0.1715, 0.7766]],\n\n         [[0.0515, 0.1785, 0.7700]],\n\n         [[0.0524, 0.1640, 0.7835]],\n\n         [[0.0521, 0.1694, 0.7785]],\n\n         [[0.0528, 0.1602, 0.7871]],\n\n         [[0.0522, 0.1671, 0.7807]],\n\n         [[0.0525, 0.1629, 0.7846]],\n\n         [[0.0522, 0.1667, 0.7810]],\n\n         [[0.0531, 0.1563, 0.7906]],\n\n         [[0.0527, 0.1603, 0.7869]],\n\n         [[0.0523, 0.1662, 0.7815]],\n\n         [[0.0528, 0.1593, 0.7879]],\n\n         [[0.0530, 0.1572, 0.7897]],\n\n         [[0.0531, 0.1560, 0.7909]],\n\n         [[0.0533, 0.1537, 0.7930]],\n\n         [[0.0536, 0.1510, 0.7954]],\n\n         [[0.0532, 0.1549, 0.7918]],\n\n         [[0.0535, 0.1520, 0.7945]],\n\n         [[0.0538, 0.1489, 0.7972]],\n\n         [[0.0535, 0.1517, 0.7948]],\n\n         [[0.0536, 0.1512, 0.7952]],\n\n         [[0.0535, 0.1522, 0.7943]],\n\n         [[0.0537, 0.1502, 0.7961]],\n\n         [[0.0536, 0.1511, 0.7953]],\n\n         [[0.0537, 0.1506, 0.7958]],\n\n         [[0.0538, 0.1491, 0.7971]],\n\n         [[0.0539, 0.1482, 0.7979]],\n\n         [[0.0539, 0.1479, 0.7982]],\n\n         [[0.0540, 0.1476, 0.7984]],\n\n         [[0.0543, 0.1444, 0.8012]],\n\n         [[0.0541, 0.1465, 0.7994]],\n\n         [[0.0542, 0.1453, 0.8004]],\n\n         [[0.0543, 0.1444, 0.8013]],\n\n         [[0.0541, 0.1462, 0.7997]],\n\n         [[0.0538, 0.1493, 0.7969]],\n\n         [[0.0535, 0.1520, 0.7945]],\n\n         [[0.0536, 0.1509, 0.7955]],\n\n         [[0.0537, 0.1498, 0.7964]],\n\n         [[0.0541, 0.1468, 0.7991]],\n\n         [[0.0539, 0.1483, 0.7978]],\n\n         [[0.0540, 0.1476, 0.7984]]]], device='cuda:0'), decoder_variables=tensor([[[[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]],\n\n         [[0.8138, 0.1862]]]], device='cuda:0'), decoder_lengths=tensor([20], device='cuda:0'), encoder_lengths=tensor([80], device='cuda:0')), x={'encoder_cat': tensor([], device='cuda:0', size=(1, 80, 0), dtype=torch.int64), 'encoder_cont': tensor([[[ 1.0000,  0.6694, -0.1568,  1.7306, -1.0000, -1.9393],\n         [ 1.0000,  0.6694, -0.1568,  1.7307, -0.9875, -2.0395],\n         [ 1.0000,  0.6694, -0.1568,  1.7307, -0.9750, -2.0482],\n         [ 1.0000,  0.6694, -0.1568,  1.7307, -0.9625, -1.8654],\n         [ 1.0000,  0.6694, -0.1568,  1.7307, -0.9500, -2.0177],\n         [ 1.0000,  0.6694, -0.1568,  1.7307, -0.9375, -1.7652],\n         [ 1.0000,  0.6694, -0.1568,  1.7308, -0.9250, -1.9524],\n         [ 1.0000,  0.6694, -0.1568,  1.7308, -0.9125, -1.9437],\n         [ 1.0000,  0.6694, -0.1568,  1.7308, -0.9000, -1.8218],\n         [ 1.0000,  0.6694, -0.1568,  1.7308, -0.8875, -1.8218],\n         [ 1.0000,  0.6694, -0.1568,  1.7308, -0.8750, -1.8044],\n         [ 1.0000,  0.6694, -0.1568,  1.7308, -0.8625, -1.1951],\n         [ 1.0000,  0.6694, -0.1568,  1.7309, -0.8500, -0.8469],\n         [ 1.0000,  0.6694, -0.1568,  1.7309, -0.8375, -0.6728],\n         [ 1.0000,  0.6694, -0.1568,  1.7309, -0.8250, -0.6162],\n         [ 1.0000,  0.6694, -0.1568,  1.7309, -0.8125, -0.7163],\n         [ 1.0000,  0.6694, -0.1568,  1.7309, -0.8000, -0.4203],\n         [ 1.0000,  0.6694, -0.1568,  1.7309, -0.7875, -0.5683],\n         [ 1.0000,  0.6694, -0.1568,  1.7310, -0.7750, -0.4203],\n         [ 1.0000,  0.6694, -0.1568,  1.7310, -0.7625, -0.6336],\n         [ 1.0000,  0.6694, -0.1568,  1.7310, -0.7500, -0.5727],\n         [ 1.0000,  0.6694, -0.1568,  1.7310, -0.7375, -0.5727],\n         [ 1.0000,  0.6694, -0.1568,  1.7310, -0.7250, -0.5509],\n         [ 1.0000,  0.6694, -0.1568,  1.7311, -0.7125, -0.3899],\n         [ 1.0000,  0.6694, -0.1568,  1.7311, -0.7000, -0.5727],\n         [ 1.0000,  0.6694, -0.1568,  1.7311, -0.6875, -0.4203],\n         [ 1.0000,  0.6694, -0.1568,  1.7311, -0.6750, -0.3159],\n         [ 1.0000,  0.6694, -0.1568,  1.7311, -0.6625, -0.4334],\n         [ 1.0000,  0.6694, -0.1568,  1.7311, -0.6500, -0.3768],\n         [ 1.0000,  0.6694, -0.1568,  1.7312, -0.6375, -0.3594],\n         [ 1.0000,  0.6694, -0.1568,  1.7312, -0.6250, -0.2985],\n         [ 1.0000,  0.6694, -0.1568,  1.7312, -0.6125, -0.2375],\n         [ 1.0000,  0.6694, -0.1568,  1.7312, -0.6000, -0.3072],\n         [ 1.0000,  0.6694, -0.1568,  1.7312, -0.5875, -0.4029],\n         [ 1.0000,  0.6694, -0.1568,  1.7312, -0.5750, -0.2811],\n         [ 1.0000,  0.6694, -0.1568,  1.7313, -0.5625, -0.1766],\n         [ 1.0000,  0.6694, -0.1568,  1.7313, -0.5500, -0.2027],\n         [ 1.0000,  0.6694, -0.1568,  1.7313, -0.5375,  0.1194],\n         [ 1.0000,  0.6694, -0.1568,  1.7313, -0.5250,  0.1020],\n         [ 1.0000,  0.6694, -0.1568,  1.7313, -0.5125,  0.2238],\n         [ 1.0000,  0.6694, -0.1568,  1.7314, -0.5000,  0.0410],\n         [ 1.0000,  0.6694, -0.1568,  1.7314, -0.4875,  0.4458],\n         [ 1.0000,  0.6694, -0.1568,  1.7314, -0.4750,  0.2673],\n         [ 1.0000,  0.6694, -0.1568,  1.7314, -0.4625,  0.5807],\n         [ 1.0000,  0.6694, -0.1568,  1.7314, -0.4500,  0.3283],\n         [ 1.0000,  0.6694, -0.1568,  1.7314, -0.4375,  0.4632],\n         [ 1.0000,  0.6694, -0.1568,  1.7315, -0.4250,  0.3283],\n         [ 1.0000,  0.6694, -0.1568,  1.7315, -0.4125,  0.7287],\n         [ 1.0000,  0.6694, -0.1568,  1.7315, -0.4000,  0.5416],\n         [ 1.0000,  0.6694, -0.1568,  1.7315, -0.3875,  0.3283],\n         [ 1.0000,  0.6694, -0.1568,  1.7315, -0.3750,  0.5720],\n         [ 1.0000,  0.6694, -0.1568,  1.7315, -0.3625,  0.6547],\n         [ 1.0000,  0.6694, -0.1568,  1.7316, -0.3500,  0.7069],\n         [ 1.0000,  0.6694, -0.1568,  1.7316, -0.3375,  0.8201],\n         [ 1.0000,  0.6694, -0.1568,  1.7316, -0.3250,  0.9812],\n         [ 1.0000,  0.6694, -0.1568,  1.7316, -0.3125,  0.7374],\n         [ 1.0000,  0.6694, -0.1568,  1.7316, -0.3000,  0.8941],\n         [ 1.0000,  0.6694, -0.1568,  1.7317, -0.2875,  1.1030],\n         [ 1.0000,  0.6694, -0.1568,  1.7317, -0.2750,  0.8985],\n         [ 1.0000,  0.6694, -0.1568,  1.7317, -0.2625,  0.9202],\n         [ 1.0000,  0.6694, -0.1568,  1.7317, -0.2500,  0.8506],\n         [ 1.0000,  0.6694, -0.1568,  1.7317, -0.2375,  0.9724],\n         [ 1.0000,  0.6694, -0.1568,  1.7317, -0.2250,  0.9028],\n         [ 1.0000,  0.6694, -0.1568,  1.7318, -0.2125,  0.9289],\n         [ 1.0000,  0.6694, -0.1568,  1.7318, -0.2000,  1.0247],\n         [ 1.0000,  0.6694, -0.1568,  1.7318, -0.1875,  1.0856],\n         [ 1.0000,  0.6694, -0.1568,  1.7318, -0.1750,  1.0987],\n         [ 1.0000,  0.6694, -0.1568,  1.7318, -0.1625,  1.1117],\n         [ 1.0000,  0.6694, -0.1568,  1.7318, -0.1500,  1.4208],\n         [ 1.0000,  0.6694, -0.1568,  1.7319, -0.1375,  1.1901],\n         [ 1.0000,  0.6694, -0.1568,  1.7319, -0.1250,  1.2945],\n         [ 1.0000,  0.6694, -0.1568,  1.7319, -0.1125,  1.3903],\n         [ 1.0000,  0.6694, -0.1568,  1.7319, -0.1000,  1.1901],\n         [ 1.0000,  0.6694, -0.1568,  1.7319, -0.0875,  0.9202],\n         [ 1.0000,  0.6694, -0.1568,  1.7320, -0.0750,  0.7418],\n         [ 1.0000,  0.6694, -0.1568,  1.7320, -0.0625,  0.7983],\n         [ 1.0000,  0.6694, -0.1568,  1.7320, -0.0500,  0.8593],\n         [ 1.0000,  0.6694, -0.1568,  1.7320, -0.0375,  1.0769],\n         [ 1.0000,  0.6694, -0.1568,  1.7320, -0.0250,  0.9463],\n         [ 1.0000,  0.6694, -0.1568,  1.7320, -0.0125,  0.9899]]],\n       device='cuda:0'), 'encoder_target': tensor([[1.7132, 1.5441, 1.5294, 1.8382, 1.5809, 2.0074, 1.6912, 1.7059, 1.9118,\n         1.9118, 1.9412, 2.9706, 3.5588, 3.8529, 3.9485, 3.7794, 4.2794, 4.0294,\n         4.2794, 3.9191, 4.0221, 4.0221, 4.0588, 4.3309, 4.0221, 4.2794, 4.4559,\n         4.2574, 4.3529, 4.3824, 4.4853, 4.5882, 4.4706, 4.3088, 4.5147, 4.6912,\n         4.6471, 5.1912, 5.1618, 5.3676, 5.0588, 5.7426, 5.4412, 5.9706, 5.5441,\n         5.7721, 5.5441, 6.2206, 5.9044, 5.5441, 5.9559, 6.0956, 6.1838, 6.3750,\n         6.6471, 6.2353, 6.5000, 6.8529, 6.5074, 6.5441, 6.4265, 6.6324, 6.5147,\n         6.5588, 6.7206, 6.8235, 6.8456, 6.8676, 7.3897, 7.0000, 7.1765, 7.3382,\n         7.0000, 6.5441, 6.2426, 6.3382, 6.4412, 6.8088, 6.5882, 6.6618]],\n       device='cuda:0'), 'encoder_lengths': tensor([80], device='cuda:0'), 'decoder_cat': tensor([], device='cuda:0', size=(1, 20, 0), dtype=torch.int64), 'decoder_cont': tensor([[[ 1.0000,  0.6694, -0.1568,  1.7321,  0.0000,  1.2379],\n         [ 1.0000,  0.6694, -0.1568,  1.7321,  0.0125,  0.9594],\n         [ 1.0000,  0.6694, -0.1568,  1.7321,  0.0250,  1.0247],\n         [ 1.0000,  0.6694, -0.1568,  1.7321,  0.0375,  1.0464],\n         [ 1.0000,  0.6694, -0.1568,  1.7321,  0.0500,  1.0987],\n         [ 1.0000,  0.6694, -0.1568,  1.7321,  0.0625,  1.2118],\n         [ 1.0000,  0.6694, -0.1568,  1.7322,  0.0750,  1.0595],\n         [ 1.0000,  0.6694, -0.1568,  1.7322,  0.0875,  1.2902],\n         [ 1.0000,  0.6694, -0.1568,  1.7322,  0.1000,  1.1161],\n         [ 1.0000,  0.6694, -0.1568,  1.7322,  0.1125,  1.2423],\n         [ 1.0000,  0.6694, -0.1568,  1.7322,  0.1250,  1.3337],\n         [ 1.0000,  0.6694, -0.1568,  1.7323,  0.1375,  1.2641],\n         [ 1.0000,  0.6694, -0.1568,  1.7323,  0.1500,  1.2597],\n         [ 1.0000,  0.6694, -0.1568,  1.7323,  0.1625,  1.3032],\n         [ 1.0000,  0.6694, -0.1568,  1.7323,  0.1750,  1.2205],\n         [ 1.0000,  0.6694, -0.1568,  1.7323,  0.1875, -1.4388],\n         [ 1.0000,  0.6694, -0.1568,  1.7323,  0.2000, -2.0220],\n         [ 1.0000,  0.6694, -0.1568,  1.7324,  0.2125, -1.9655],\n         [ 1.0000,  0.6694, -0.1568,  1.7324,  0.2250, -2.0830],\n         [ 1.0000,  0.6694, -0.1568,  1.7324,  0.2375, -1.9872]]],\n       device='cuda:0'), 'decoder_target': tensor([[7.0809, 6.6103, 6.7206, 6.7574, 6.8456, 7.0368, 6.7794, 7.1691, 6.8750,\n         7.0882, 7.2426, 7.1250, 7.1176, 7.1912, 7.0515, 2.5588, 1.5735, 1.6691,\n         1.4706, 1.6324]], device='cuda:0'), 'decoder_lengths': tensor([20], device='cuda:0'), 'decoder_time_idx': tensor([[196072, 196073, 196074, 196075, 196076, 196077, 196078, 196079, 196080,\n         196081, 196082, 196083, 196084, 196085, 196086, 196087, 196088, 196089,\n         196090, 196091]], device='cuda:0'), 'groups': tensor([[0]], device='cuda:0'), 'target_scale': tensor([[4.9895, 1.6894]], device='cuda:0')}, index=None, decoder_lengths=None, y=None)"}
