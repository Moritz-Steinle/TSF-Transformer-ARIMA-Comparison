{"label":"InfluxDB_r=4h_e=100_pl=20_hst=0","mean_squared_error":-1,"runtimes":"Training: 17.75 seconds","length_train_dataset":558,"length_test_dataset":1,"parameters":"Epochs: 100, hyperparameters: Hyperparamters(gradient_clip_val=0.09050490030726796, hidden_continuous_size=12, dropout=0.22288661702971777, attention_head_size=2, learning_rate=0.6336776189720053, accelerator='auto', hidden_size=None)","prediction":"Prediction(output=Output(prediction=tensor([[[4.7110, 4.4659, 4.2941, 3.2547, 3.4376, 7.1871, 3.0130],\n         [4.6912, 4.4596, 4.2862, 3.2370, 3.4155, 7.1769, 2.9776],\n         [4.6868, 4.4568, 4.2846, 3.2313, 3.4095, 7.1707, 2.9632],\n         [4.6870, 4.4555, 4.2862, 3.2321, 3.4095, 7.1680, 2.9588],\n         [4.6882, 4.4540, 4.2900, 3.2362, 3.4115, 7.1665, 2.9591],\n         [4.6894, 4.4522, 4.2951, 3.2418, 3.4140, 7.1654, 2.9615],\n         [4.6902, 4.4500, 4.3008, 3.2479, 3.4163, 7.1643, 2.9647],\n         [4.6908, 4.4476, 4.3069, 3.2541, 3.4184, 7.1631, 2.9683],\n         [4.6913, 4.4451, 4.3130, 3.2604, 3.4203, 7.1616, 2.9719],\n         [4.6916, 4.4427, 4.3191, 3.2664, 3.4219, 7.1599, 2.9755],\n         [4.6919, 4.4404, 4.3252, 3.2724, 3.4235, 7.1580, 2.9789],\n         [4.6922, 4.4381, 4.3312, 3.2783, 3.4250, 7.1558, 2.9822],\n         [4.6925, 4.4360, 4.3372, 3.2840, 3.4265, 7.1535, 2.9855],\n         [4.6928, 4.4339, 4.3431, 3.2897, 3.4279, 7.1510, 2.9886],\n         [4.6931, 4.4319, 4.3489, 3.2953, 3.4294, 7.1483, 2.9917],\n         [4.6934, 4.4300, 4.3547, 3.3008, 3.4309, 7.1454, 2.9947],\n         [4.6938, 4.4282, 4.3605, 3.3063, 3.4325, 7.1424, 2.9977],\n         [4.6942, 4.4265, 4.3662, 3.3118, 3.4341, 7.1392, 3.0007],\n         [4.6947, 4.4249, 4.3720, 3.3172, 3.4358, 7.1359, 3.0037],\n         [4.6951, 4.4233, 4.3777, 3.3226, 3.4375, 7.1326, 3.0067]]],\n       device='cuda:0'), encoder_attention=tensor([[[[0.0016, 0.0418, 0.0414,  ..., 0.0007, 0.0007, 0.0006],\n          [0.0059, 0.0257, 0.0264,  ..., 0.0063, 0.0061, 0.0063],\n          [0.0027, 0.0040, 0.0041,  ..., 0.0207, 0.0209, 0.0203],\n          [0.1210, 0.0045, 0.0049,  ..., 0.0007, 0.0007, 0.0007]],\n\n         [[0.0017, 0.0415, 0.0412,  ..., 0.0007, 0.0007, 0.0007],\n          [0.0055, 0.0256, 0.0264,  ..., 0.0063, 0.0061, 0.0062],\n          [0.0027, 0.0040, 0.0040,  ..., 0.0201, 0.0202, 0.0196],\n          [0.1224, 0.0044, 0.0048,  ..., 0.0006, 0.0007, 0.0006]],\n\n         [[0.0017, 0.0414, 0.0411,  ..., 0.0007, 0.0007, 0.0007],\n          [0.0053, 0.0255, 0.0262,  ..., 0.0062, 0.0060, 0.0062],\n          [0.0027, 0.0039, 0.0040,  ..., 0.0196, 0.0198, 0.0191],\n          [0.1231, 0.0044, 0.0047,  ..., 0.0006, 0.0007, 0.0006]],\n\n         ...,\n\n         [[0.0016, 0.0415, 0.0411,  ..., 0.0007, 0.0007, 0.0007],\n          [0.0043, 0.0228, 0.0235,  ..., 0.0055, 0.0053, 0.0055],\n          [0.0021, 0.0032, 0.0032,  ..., 0.0149, 0.0151, 0.0145],\n          [0.1217, 0.0035, 0.0038,  ..., 0.0006, 0.0006, 0.0006]],\n\n         [[0.0016, 0.0414, 0.0411,  ..., 0.0007, 0.0007, 0.0007],\n          [0.0042, 0.0226, 0.0233,  ..., 0.0055, 0.0053, 0.0054],\n          [0.0020, 0.0032, 0.0032,  ..., 0.0147, 0.0148, 0.0143],\n          [0.1215, 0.0035, 0.0038,  ..., 0.0006, 0.0006, 0.0006]],\n\n         [[0.0016, 0.0414, 0.0410,  ..., 0.0007, 0.0007, 0.0007],\n          [0.0041, 0.0225, 0.0231,  ..., 0.0054, 0.0052, 0.0054],\n          [0.0020, 0.0031, 0.0032,  ..., 0.0145, 0.0146, 0.0141],\n          [0.1214, 0.0034, 0.0037,  ..., 0.0006, 0.0006, 0.0006]]]],\n       device='cuda:0'), decoder_attention=tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0006, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0097, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0195, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0006, 0.0006, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0097, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0191, 0.0187, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0013, 0.0013, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         ...,\n\n         [[0.0005, 0.0005, 0.0005,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0086, 0.0083, 0.0082,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0148, 0.0144, 0.0144,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0012, 0.0012, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0000, 0.0000],\n          [0.0086, 0.0083, 0.0081,  ..., 0.0076, 0.0000, 0.0000],\n          [0.0145, 0.0142, 0.0141,  ..., 0.0144, 0.0000, 0.0000],\n          [0.0012, 0.0012, 0.0012,  ..., 0.0014, 0.0000, 0.0000]],\n\n         [[0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0000],\n          [0.0086, 0.0082, 0.0081,  ..., 0.0076, 0.0076, 0.0000],\n          [0.0143, 0.0140, 0.0139,  ..., 0.0142, 0.0142, 0.0000],\n          [0.0012, 0.0012, 0.0012,  ..., 0.0014, 0.0014, 0.0000]]]],\n       device='cuda:0'), static_variables=tensor([[[0.6543, 0.2872, 0.0585]]], device='cuda:0'), encoder_variables=tensor([[[[0.0618, 0.2216, 0.7166]],\n\n         [[0.3129, 0.6289, 0.0582]],\n\n         [[0.3117, 0.6301, 0.0582]],\n\n         [[0.3193, 0.6227, 0.0581]],\n\n         [[0.3204, 0.6216, 0.0580]],\n\n         [[0.3200, 0.6219, 0.0580]],\n\n         [[0.3171, 0.6248, 0.0581]],\n\n         [[0.3191, 0.6228, 0.0581]],\n\n         [[0.2842, 0.6568, 0.0590]],\n\n         [[0.2788, 0.6620, 0.0592]],\n\n         [[0.2748, 0.6658, 0.0594]],\n\n         [[0.2807, 0.6601, 0.0591]],\n\n         [[0.2927, 0.6486, 0.0588]],\n\n         [[0.2975, 0.6439, 0.0586]],\n\n         [[0.3039, 0.6377, 0.0584]],\n\n         [[0.3028, 0.6387, 0.0585]],\n\n         [[0.3115, 0.6303, 0.0582]],\n\n         [[0.3172, 0.6247, 0.0581]],\n\n         [[0.3250, 0.6171, 0.0579]],\n\n         [[0.3328, 0.6094, 0.0578]],\n\n         [[0.3417, 0.6007, 0.0576]],\n\n         [[0.3611, 0.5815, 0.0573]],\n\n         [[0.3711, 0.5717, 0.0572]],\n\n         [[0.4143, 0.5290, 0.0568]],\n\n         [[0.4531, 0.4903, 0.0566]],\n\n         [[0.4777, 0.4657, 0.0566]],\n\n         [[0.6823, 0.2579, 0.0598]],\n\n         [[0.7503, 0.1844, 0.0653]],\n\n         [[0.6867, 0.0600, 0.2532]],\n\n         [[0.1401, 0.0732, 0.7866]],\n\n         [[0.2010, 0.0633, 0.7356]],\n\n         [[0.2107, 0.0625, 0.7267]],\n\n         [[0.2870, 0.0586, 0.6544]],\n\n         [[0.2330, 0.0610, 0.7060]],\n\n         [[0.1644, 0.0680, 0.7676]],\n\n         [[0.1476, 0.0713, 0.7811]],\n\n         [[0.1099, 0.0858, 0.8043]],\n\n         [[0.1234, 0.0789, 0.7977]],\n\n         [[0.1429, 0.0725, 0.7846]],\n\n         [[0.5920, 0.0573, 0.3507]],\n\n         [[0.3748, 0.0569, 0.5682]],\n\n         [[0.2784, 0.0589, 0.6627]],\n\n         [[0.1515, 0.0704, 0.7780]],\n\n         [[0.2355, 0.0608, 0.7037]],\n\n         [[0.2040, 0.0631, 0.7329]],\n\n         [[0.2298, 0.0612, 0.7090]],\n\n         [[0.2090, 0.0626, 0.7283]],\n\n         [[0.1539, 0.0699, 0.7762]],\n\n         [[0.1731, 0.0666, 0.7603]],\n\n         [[0.1883, 0.0647, 0.7471]],\n\n         [[0.4569, 0.4865, 0.0566]],\n\n         [[0.4598, 0.4836, 0.0566]],\n\n         [[0.4646, 0.4788, 0.0566]],\n\n         [[0.4722, 0.4712, 0.0566]],\n\n         [[0.4729, 0.4705, 0.0566]],\n\n         [[0.4734, 0.4700, 0.0566]],\n\n         [[0.4798, 0.4636, 0.0566]],\n\n         [[0.4800, 0.4634, 0.0566]],\n\n         [[0.4891, 0.4543, 0.0566]],\n\n         [[0.4890, 0.4545, 0.0566]],\n\n         [[0.4924, 0.4510, 0.0566]],\n\n         [[0.4977, 0.4458, 0.0566]],\n\n         [[0.5030, 0.4404, 0.0566]],\n\n         [[0.5084, 0.4350, 0.0566]],\n\n         [[0.5148, 0.4285, 0.0566]],\n\n         [[0.5213, 0.4221, 0.0567]],\n\n         [[0.5258, 0.4175, 0.0567]],\n\n         [[0.5330, 0.4103, 0.0567]],\n\n         [[0.5363, 0.4070, 0.0568]],\n\n         [[0.5496, 0.3936, 0.0568]],\n\n         [[0.5618, 0.3813, 0.0570]],\n\n         [[0.5704, 0.3725, 0.0570]],\n\n         [[0.5742, 0.3688, 0.0571]],\n\n         [[0.5823, 0.3606, 0.0572]],\n\n         [[0.5878, 0.3550, 0.0573]],\n\n         [[0.6141, 0.3282, 0.0577]],\n\n         [[0.6166, 0.3257, 0.0578]],\n\n         [[0.6380, 0.3038, 0.0582]],\n\n         [[0.6318, 0.3101, 0.0581]],\n\n         [[0.6513, 0.2901, 0.0586]]]], device='cuda:0'), decoder_variables=tensor([[[[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]],\n\n         [[0.8798, 0.1202]]]], device='cuda:0'), decoder_lengths=tensor([20], device='cuda:0'), encoder_lengths=tensor([80], device='cuda:0')), x={'encoder_cat': tensor([], device='cuda:0', size=(1, 80, 0), dtype=torch.int64), 'encoder_cont': tensor([[[ 1.0000, -0.1714,  0.2062,  1.2211, -1.0000,  1.5713],\n         [ 1.0000, -0.1714,  0.2062,  1.2275, -0.9875, -1.2526],\n         [ 1.0000, -0.1714,  0.2062,  1.2340, -0.9750, -1.1880],\n         [ 1.0000, -0.1714,  0.2062,  1.2404, -0.9625, -1.2778],\n         [ 1.0000, -0.1714,  0.2062,  1.2468, -0.9500, -1.2526],\n         [ 1.0000, -0.1714,  0.2062,  1.2532, -0.9375, -1.1988],\n         [ 1.0000, -0.1714,  0.2062,  1.2597, -0.9250, -1.0910],\n         [ 1.0000, -0.1714,  0.2062,  1.2661, -0.9125, -1.0766],\n         [ 1.0000, -0.1714,  0.2062,  1.2725, -0.9000, -0.2862],\n         [ 1.0000, -0.1714,  0.2062,  1.2790, -0.8875, -0.0957],\n         [ 1.0000, -0.1714,  0.2062,  1.2854, -0.8750,  0.0659],\n         [ 1.0000, -0.1714,  0.2062,  1.2918, -0.8625,  0.0659],\n         [ 1.0000, -0.1714,  0.2062,  1.2982, -0.8500, -0.0598],\n         [ 1.0000, -0.1714,  0.2062,  1.3047, -0.8375, -0.0419],\n         [ 1.0000, -0.1714,  0.2062,  1.3111, -0.8250, -0.0598],\n         [ 1.0000, -0.1714,  0.2062,  1.3175, -0.8125,  0.1522],\n         [ 1.0000, -0.1714,  0.2062,  1.3239, -0.8000,  0.1019],\n         [ 1.0000, -0.1714,  0.2062,  1.3304, -0.7875,  0.1665],\n         [ 1.0000, -0.1714,  0.2062,  1.3368, -0.7750,  0.1593],\n         [ 1.0000, -0.1714,  0.2062,  1.3432, -0.7625,  0.1809],\n         [ 1.0000, -0.1714,  0.2062,  1.3497, -0.7500,  0.2456],\n         [ 1.0000, -0.1714,  0.2062,  1.3561, -0.7375,  0.4971],\n         [ 1.0000, -0.1714,  0.2062,  1.3625, -0.7250,  0.4468],\n         [ 1.0000, -0.1714,  0.2062,  1.3689, -0.7125,  0.6336],\n         [ 1.0000, -0.1714,  0.2062,  1.3754, -0.7000,  0.6839],\n         [ 1.0000, -0.1714,  0.2062,  1.3818, -0.6875,  0.6839],\n         [ 1.0000, -0.1714,  0.2062,  1.3882, -0.6750,  0.8599],\n         [ 1.0000, -0.1714,  0.2062,  1.3946, -0.6625,  0.8851],\n         [ 1.0000, -0.1714,  0.2062,  1.4011, -0.6500,  0.9965],\n         [ 1.0000, -0.1714,  0.2062,  1.4075, -0.6375,  1.2228],\n         [ 1.0000, -0.1714,  0.2062,  1.4139, -0.6250,  1.1510],\n         [ 1.0000, -0.1714,  0.2062,  1.4203, -0.6125,  1.1546],\n         [ 1.0000, -0.1714,  0.2062,  1.4268, -0.6000,  1.1150],\n         [ 1.0000, -0.1714,  0.2062,  1.4332, -0.5875,  1.1582],\n         [ 1.0000, -0.1714,  0.2062,  1.4396, -0.5750,  1.2588],\n         [ 1.0000, -0.1714,  0.2062,  1.4461, -0.5625,  1.3198],\n         [ 1.0000, -0.1714,  0.2062,  1.4525, -0.5500,  1.5857],\n         [ 1.0000, -0.1714,  0.2062,  1.4589, -0.5375,  1.4815],\n         [ 1.0000, -0.1714,  0.2062,  1.4653, -0.5250,  1.3953],\n         [ 1.0000, -0.1714,  0.2062,  1.4718, -0.5125,  1.0252],\n         [ 1.0000, -0.1714,  0.2062,  1.4782, -0.5000,  1.1222],\n         [ 1.0000, -0.1714,  0.2062,  1.4846, -0.4875,  1.1941],\n         [ 1.0000, -0.1714,  0.2062,  1.4910, -0.4750,  1.4348],\n         [ 1.0000, -0.1714,  0.2062,  1.4975, -0.4625,  1.2588],\n         [ 1.0000, -0.1714,  0.2062,  1.5039, -0.4500,  1.3198],\n         [ 1.0000, -0.1714,  0.2062,  1.5103, -0.4375,  1.2875],\n         [ 1.0000, -0.1714,  0.2062,  1.5168, -0.4250,  1.3342],\n         [ 1.0000, -0.1714,  0.2062,  1.5232, -0.4125,  1.5138],\n         [ 1.0000, -0.1714,  0.2062,  1.5296, -0.4000,  1.4528],\n         [ 1.0000, -0.1714,  0.2062,  1.5360, -0.3875,  1.4204],\n         [ 1.0000, -0.1714,  0.2062,  1.5425, -0.3750, -1.2562],\n         [ 1.0000, -0.1714,  0.2062,  1.5489, -0.3625, -1.3065],\n         [ 1.0000, -0.1714,  0.2062,  1.5553, -0.3500, -1.1772],\n         [ 1.0000, -0.1714,  0.2062,  1.5617, -0.3375, -1.0155],\n         [ 1.0000, -0.1714,  0.2062,  1.5682, -0.3250, -1.1125],\n         [ 1.0000, -0.1714,  0.2062,  1.5746, -0.3125, -1.2455],\n         [ 1.0000, -0.1714,  0.2062,  1.5810, -0.3000, -1.1161],\n         [ 1.0000, -0.1714,  0.2062,  1.5874, -0.2875, -1.2419],\n         [ 1.0000, -0.1714,  0.2062,  1.5939, -0.2750, -1.0514],\n         [ 1.0000, -0.1714,  0.2062,  1.6003, -0.2625, -1.1556],\n         [ 1.0000, -0.1714,  0.2062,  1.6067, -0.2500, -1.1556],\n         [ 1.0000, -0.1714,  0.2062,  1.6132, -0.2375, -1.1053],\n         [ 1.0000, -0.1714,  0.2062,  1.6196, -0.2250, -1.0622],\n         [ 1.0000, -0.1714,  0.2062,  1.6260, -0.2125, -1.0263],\n         [ 1.0000, -0.1714,  0.2062,  1.6324, -0.2000, -0.9760],\n         [ 1.0000, -0.1714,  0.2062,  1.6389, -0.1875, -0.9329],\n         [ 1.0000, -0.1714,  0.2062,  1.6453, -0.1750, -0.9221],\n         [ 1.0000, -0.1714,  0.2062,  1.6517, -0.1625, -0.8754],\n         [ 1.0000, -0.1714,  0.2062,  1.6581, -0.1500, -0.8826],\n         [ 1.0000, -0.1714,  0.2062,  1.6646, -0.1375, -0.7748],\n         [ 1.0000, -0.1714,  0.2062,  1.6710, -0.1250, -0.6958],\n         [ 1.0000, -0.1714,  0.2062,  1.6774, -0.1125, -0.6562],\n         [ 1.0000, -0.1714,  0.2062,  1.6839, -0.1000, -0.6598],\n         [ 1.0000, -0.1714,  0.2062,  1.6903, -0.0875, -0.6275],\n         [ 1.0000, -0.1714,  0.2062,  1.6967, -0.0750, -0.6167],\n         [ 1.0000, -0.1714,  0.2062,  1.7031, -0.0625, -0.4622],\n         [ 1.0000, -0.1714,  0.2062,  1.7096, -0.0500, -0.4730],\n         [ 1.0000, -0.1714,  0.2062,  1.7160, -0.0375, -0.3688],\n         [ 1.0000, -0.1714,  0.2062,  1.7224, -0.0250, -0.4299],\n         [ 1.0000, -0.1714,  0.2062,  1.7288, -0.0125, -0.3401]]],\n       device='cuda:0'), 'encoder_target': tensor([[7.6451, 1.5363, 1.6762, 1.4819, 1.5363, 1.6528, 1.8860, 1.9171, 3.6269,\n         4.0389, 4.3886, 4.3886, 4.1166, 4.1554, 4.1166, 4.5751, 4.4663, 4.6062,\n         4.5907, 4.6373, 4.7772, 5.3212, 5.2124, 5.6166, 5.7254, 5.7254, 6.1062,\n         6.1606, 6.4016, 6.8912, 6.7358, 6.7435, 6.6580, 6.7513, 6.9689, 7.1010,\n         7.6762, 7.4508, 7.2642, 6.4637, 6.6736, 6.8290, 7.3497, 6.9689, 7.1010,\n         7.0311, 7.1321, 7.5207, 7.3886, 7.3187, 1.5285, 1.4197, 1.6995, 2.0492,\n         1.8394, 1.5518, 1.8316, 1.5596, 1.9715, 1.7461, 1.7461, 1.8549, 1.9482,\n         2.0259, 2.1347, 2.2280, 2.2513, 2.3523, 2.3368, 2.5699, 2.7409, 2.8264,\n         2.8187, 2.8886, 2.9119, 3.2461, 3.2228, 3.4482, 3.3161, 3.5104]],\n       device='cuda:0'), 'encoder_lengths': tensor([80], device='cuda:0'), 'decoder_cat': tensor([], device='cuda:0', size=(1, 20, 0), dtype=torch.int64), 'decoder_cont': tensor([[[ 1.0000, -0.1714,  0.2062,  1.7353,  0.0000, -0.4263],\n         [ 1.0000, -0.1714,  0.2062,  1.7417,  0.0125, -0.2574],\n         [ 1.0000, -0.1714,  0.2062,  1.7481,  0.0250, -0.4658],\n         [ 1.0000, -0.1714,  0.2062,  1.7545,  0.0375, -0.3437],\n         [ 1.0000, -0.1714,  0.2062,  1.7610,  0.0500, -0.4802],\n         [ 1.0000, -0.1714,  0.2062,  1.7674,  0.0625, -0.2934],\n         [ 1.0000, -0.1714,  0.2062,  1.7738,  0.0750, -0.4191],\n         [ 1.0000, -0.1714,  0.2062,  1.7803,  0.0875, -0.4083],\n         [ 1.0000, -0.1714,  0.2062,  1.7867,  0.1000, -0.3652],\n         [ 1.0000, -0.1714,  0.2062,  1.7931,  0.1125, -0.2898],\n         [ 1.0000, -0.1714,  0.2062,  1.7995,  0.1250, -0.2790],\n         [ 1.0000, -0.1714,  0.2062,  1.8060,  0.1375, -0.2826],\n         [ 1.0000, -0.1714,  0.2062,  1.8124,  0.1500, -0.2215],\n         [ 1.0000, -0.1714,  0.2062,  1.8188,  0.1625, -0.2035],\n         [ 1.0000, -0.1714,  0.2062,  1.8252,  0.1750, -0.1640],\n         [ 1.0000, -0.1714,  0.2062,  1.8317,  0.1875, -0.1209],\n         [ 1.0000, -0.1714,  0.2062,  1.8381,  0.2000, -0.0706],\n         [ 1.0000, -0.1714,  0.2062,  1.8445,  0.2125, -0.0598],\n         [ 1.0000, -0.1714,  0.2062,  1.8510,  0.2250,  0.0156],\n         [ 1.0000, -0.1714,  0.2062,  1.8574,  0.2375,  0.0084]]],\n       device='cuda:0'), 'decoder_target': tensor([[3.3238, 3.6891, 3.2383, 3.5026, 3.2073, 3.6114, 3.3394, 3.3627, 3.4560,\n         3.6192, 3.6425, 3.6347, 3.7668, 3.8057, 3.8912, 3.9845, 4.0933, 4.1166,\n         4.2798, 4.2642]], device='cuda:0'), 'decoder_lengths': tensor([20], device='cuda:0'), 'decoder_time_idx': tensor([[539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552,\n         553, 554, 555, 556, 557, 558]], device='cuda:0'), 'groups': tensor([[0]], device='cuda:0'), 'target_scale': tensor([[4.2460, 2.1632]], device='cuda:0')}, index=None, decoder_lengths=None, y=None)"}
