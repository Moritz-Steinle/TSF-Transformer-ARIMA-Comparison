{"label":"InfluxDB_r=2h-chained_atIncline_e=1000_pl=20_hst=0","mean_squared_error":-1,"runtimes":"Training: 132.47 seconds","length_train_dataset":5446,"length_test_dataset":1,"parameters":"Epochs: 1000, hyperparameters: Hyperparamters(gradient_clip_val=6.9953515571, hidden_continuous_size=40, dropout=0.1558743686, attention_head_size=3, learning_rate=0.0039810717, accelerator='auto', hidden_size=70)","prediction":"Prediction(output=Output(prediction=tensor([[[2.8645, 5.3209, 6.6990, 6.8708, 6.9807, 7.2240, 7.3426],\n         [2.7867, 5.3320, 6.7616, 6.9682, 7.0476, 7.2598, 7.3796],\n         [2.7123, 5.3187, 6.8078, 7.0438, 7.1063, 7.3026, 7.4021],\n         [2.6262, 5.2838, 6.8401, 7.1064, 7.1655, 7.3501, 7.4266],\n         [2.5154, 5.2227, 6.8586, 7.1641, 7.2311, 7.4036, 7.4582],\n         [2.3731, 5.1315, 6.8628, 7.2215, 7.3057, 7.4623, 7.4992],\n         [2.2022, 5.0112, 6.8507, 7.2780, 7.3858, 7.5216, 7.5489],\n         [2.0175, 4.8718, 6.8208, 7.3281, 7.4625, 7.5747, 7.6028],\n         [1.8354, 4.7260, 6.7741, 7.3657, 7.5279, 7.6179, 7.6555],\n         [1.6580, 4.5769, 6.7113, 7.3901, 7.5817, 7.6530, 7.7057],\n         [1.4719, 4.4144, 6.6296, 7.4041, 7.6289, 7.6843, 7.7567],\n         [1.2563, 4.2203, 6.5210, 7.4090, 7.6744, 7.7153, 7.8139],\n         [0.9897, 3.9726, 6.3716, 7.4022, 7.7202, 7.7481, 7.8818],\n         [0.6559, 3.6492, 6.1638, 7.3750, 7.7634, 7.7849, 7.9635],\n         [0.2545, 3.2380, 5.8798, 7.3120, 7.7943, 7.8263, 8.0569],\n         [0.0000, 2.7544, 5.5179, 7.1977, 7.8001, 7.8719, 8.1530],\n         [0.0000, 2.5161, 5.0758, 7.0741, 7.8732, 8.0809, 8.1104],\n         [0.0000, 3.2066, 4.6847, 6.7346, 7.7120, 8.0716, 7.5572],\n         [0.0000, 2.9108, 4.3245, 6.4924, 7.6745, 8.1690, 7.6149],\n         [0.0000, 2.6325, 3.9384, 6.1818, 7.5831, 8.2533, 7.6319]]],\n       device='cuda:0'), encoder_attention=tensor([[[[0.0064, 0.0056, 0.0052,  ..., 0.0169, 0.0168, 0.0168],\n          [0.0124, 0.0138, 0.0144,  ..., 0.0025, 0.0030, 0.0028],\n          [0.0497, 0.0363, 0.0282,  ..., 0.0107, 0.0114, 0.0112],\n          [0.0314, 0.0299, 0.0280,  ..., 0.0074, 0.0079, 0.0077]],\n\n         [[0.0064, 0.0057, 0.0053,  ..., 0.0169, 0.0168, 0.0168],\n          [0.0123, 0.0139, 0.0146,  ..., 0.0020, 0.0023, 0.0022],\n          [0.0479, 0.0354, 0.0278,  ..., 0.0099, 0.0106, 0.0104],\n          [0.0318, 0.0306, 0.0286,  ..., 0.0065, 0.0070, 0.0069]],\n\n         [[0.0061, 0.0054, 0.0051,  ..., 0.0171, 0.0169, 0.0170],\n          [0.0123, 0.0141, 0.0149,  ..., 0.0018, 0.0021, 0.0020],\n          [0.0465, 0.0346, 0.0274,  ..., 0.0096, 0.0103, 0.0101],\n          [0.0324, 0.0312, 0.0292,  ..., 0.0061, 0.0066, 0.0065]],\n\n         ...,\n\n         [[0.0015, 0.0011, 0.0010,  ..., 0.0190, 0.0188, 0.0188],\n          [0.0176, 0.0225, 0.0242,  ..., 0.0011, 0.0012, 0.0011],\n          [0.0323, 0.0217, 0.0156,  ..., 0.0099, 0.0103, 0.0102],\n          [0.0273, 0.0264, 0.0256,  ..., 0.0059, 0.0061, 0.0061]],\n\n         [[0.0014, 0.0010, 0.0009,  ..., 0.0190, 0.0187, 0.0188],\n          [0.0182, 0.0229, 0.0245,  ..., 0.0013, 0.0014, 0.0014],\n          [0.0303, 0.0204, 0.0148,  ..., 0.0098, 0.0101, 0.0100],\n          [0.0269, 0.0260, 0.0251,  ..., 0.0060, 0.0062, 0.0062]],\n\n         [[0.0014, 0.0010, 0.0008,  ..., 0.0189, 0.0187, 0.0187],\n          [0.0187, 0.0231, 0.0246,  ..., 0.0017, 0.0018, 0.0018],\n          [0.0286, 0.0194, 0.0140,  ..., 0.0097, 0.0099, 0.0099],\n          [0.0262, 0.0251, 0.0242,  ..., 0.0062, 0.0063, 0.0063]]]],\n       device='cuda:0'), decoder_attention=tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0109, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0007, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0202, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0028, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0108, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0006, 0.0006, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0196, 0.0200, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0024, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         ...,\n\n         [[0.0063, 0.0073, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0012, 0.0011, 0.0010,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0213, 0.0220, 0.0220,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0067, 0.0066, 0.0067,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0061, 0.0071, 0.0076,  ..., 0.0037, 0.0000, 0.0000],\n          [0.0016, 0.0015, 0.0014,  ..., 0.0025, 0.0000, 0.0000],\n          [0.0210, 0.0217, 0.0217,  ..., 0.0329, 0.0000, 0.0000],\n          [0.0073, 0.0073, 0.0073,  ..., 0.0118, 0.0000, 0.0000]],\n\n         [[0.0059, 0.0069, 0.0074,  ..., 0.0035, 0.0031, 0.0000],\n          [0.0022, 0.0020, 0.0019,  ..., 0.0031, 0.0034, 0.0000],\n          [0.0207, 0.0215, 0.0214,  ..., 0.0305, 0.0300, 0.0000],\n          [0.0080, 0.0080, 0.0081,  ..., 0.0132, 0.0139, 0.0000]]]],\n       device='cuda:0'), static_variables=tensor([[[0.0479, 0.2099, 0.7421]]], device='cuda:0'), encoder_variables=tensor([[[[0.2568, 0.0504, 0.6927]],\n\n         [[0.2571, 0.0504, 0.6924]],\n\n         [[0.2575, 0.0504, 0.6921]],\n\n         [[0.2581, 0.0504, 0.6916]],\n\n         [[0.2583, 0.0504, 0.6913]],\n\n         [[0.2590, 0.0504, 0.6907]],\n\n         [[0.2591, 0.0504, 0.6905]],\n\n         [[0.2595, 0.0503, 0.6901]],\n\n         [[0.2601, 0.0503, 0.6896]],\n\n         [[0.2605, 0.0503, 0.6892]],\n\n         [[0.2609, 0.0503, 0.6888]],\n\n         [[0.2626, 0.0503, 0.6872]],\n\n         [[0.2640, 0.0502, 0.6858]],\n\n         [[0.2651, 0.0502, 0.6847]],\n\n         [[0.2657, 0.0502, 0.6841]],\n\n         [[0.2658, 0.0502, 0.6840]],\n\n         [[0.2674, 0.0501, 0.6825]],\n\n         [[0.2673, 0.0501, 0.6826]],\n\n         [[0.2683, 0.0501, 0.6816]],\n\n         [[0.2679, 0.0501, 0.6819]],\n\n         [[0.2686, 0.0501, 0.6813]],\n\n         [[0.2691, 0.0501, 0.6808]],\n\n         [[0.2697, 0.0501, 0.6803]],\n\n         [[0.2708, 0.0501, 0.6791]],\n\n         [[0.2705, 0.0501, 0.6794]],\n\n         [[0.2716, 0.0500, 0.6783]],\n\n         [[0.2726, 0.0500, 0.6774]],\n\n         [[0.2725, 0.0500, 0.6775]],\n\n         [[0.2732, 0.0500, 0.6767]],\n\n         [[0.2738, 0.0500, 0.6762]],\n\n         [[0.2746, 0.0500, 0.6754]],\n\n         [[0.2754, 0.0500, 0.6747]],\n\n         [[0.2755, 0.0500, 0.6745]],\n\n         [[0.2756, 0.0500, 0.6745]],\n\n         [[0.2766, 0.0499, 0.6734]],\n\n         [[0.2776, 0.0499, 0.6724]],\n\n         [[0.2780, 0.0499, 0.6721]],\n\n         [[0.2802, 0.0499, 0.6700]],\n\n         [[0.2806, 0.0499, 0.6695]],\n\n         [[0.2818, 0.0498, 0.6684]],\n\n         [[0.2813, 0.0498, 0.6689]],\n\n         [[0.2841, 0.0498, 0.6661]],\n\n         [[0.2836, 0.0498, 0.6666]],\n\n         [[0.2859, 0.0498, 0.6643]],\n\n         [[0.2850, 0.0498, 0.6652]],\n\n         [[0.2863, 0.0497, 0.6639]],\n\n         [[0.2861, 0.0497, 0.6642]],\n\n         [[0.2890, 0.0497, 0.6613]],\n\n         [[0.2884, 0.0497, 0.6619]],\n\n         [[0.2877, 0.0497, 0.6626]],\n\n         [[0.2897, 0.0497, 0.6606]],\n\n         [[0.2907, 0.0497, 0.6596]],\n\n         [[0.2916, 0.0497, 0.6587]],\n\n         [[0.2927, 0.0496, 0.6577]],\n\n         [[0.2936, 0.0496, 0.6568]],\n\n         [[0.2934, 0.0496, 0.6570]],\n\n         [[0.2945, 0.0496, 0.6559]],\n\n         [[0.2951, 0.0496, 0.6553]],\n\n         [[0.2956, 0.0496, 0.6548]],\n\n         [[0.2961, 0.0496, 0.6543]],\n\n         [[0.2965, 0.0496, 0.6539]],\n\n         [[0.2972, 0.0496, 0.6532]],\n\n         [[0.2977, 0.0496, 0.6528]],\n\n         [[0.2982, 0.0496, 0.6522]],\n\n         [[0.2987, 0.0495, 0.6517]],\n\n         [[0.2991, 0.0495, 0.6513]],\n\n         [[0.2996, 0.0495, 0.6509]],\n\n         [[0.3001, 0.0495, 0.6504]],\n\n         [[0.2991, 0.0495, 0.6513]],\n\n         [[0.3008, 0.0495, 0.6497]],\n\n         [[0.3008, 0.0495, 0.6497]],\n\n         [[0.3007, 0.0495, 0.6498]],\n\n         [[0.3023, 0.0495, 0.6482]],\n\n         [[0.3034, 0.0495, 0.6471]],\n\n         [[0.3038, 0.0495, 0.6467]],\n\n         [[0.3045, 0.0495, 0.6461]],\n\n         [[0.3050, 0.0495, 0.6455]],\n\n         [[0.3052, 0.0495, 0.6454]],\n\n         [[0.3060, 0.0494, 0.6445]],\n\n         [[0.3064, 0.0494, 0.6441]]]], device='cuda:0'), decoder_variables=tensor([[[[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.8200, 0.1800]],\n\n         [[0.6390, 0.3610]],\n\n         [[0.1245, 0.8755]],\n\n         [[0.1245, 0.8755]],\n\n         [[0.1245, 0.8755]]]], device='cuda:0'), decoder_lengths=tensor([20], device='cuda:0'), encoder_lengths=tensor([80], device='cuda:0')), x={'encoder_cat': tensor([], device='cuda:0', size=(1, 80, 0), dtype=torch.int64), 'encoder_cont': tensor([[[ 1.0000,  0.6742, -0.1535,  1.6813, -1.0000, -1.9393],\n         [ 1.0000,  0.6742, -0.1535,  1.6819, -0.9875, -2.0395],\n         [ 1.0000,  0.6742, -0.1535,  1.6826, -0.9750, -2.0482],\n         [ 1.0000,  0.6742, -0.1535,  1.6832, -0.9625, -1.8654],\n         [ 1.0000,  0.6742, -0.1535,  1.6839, -0.9500, -2.0177],\n         [ 1.0000,  0.6742, -0.1535,  1.6845, -0.9375, -1.7652],\n         [ 1.0000,  0.6742, -0.1535,  1.6851, -0.9250, -1.9524],\n         [ 1.0000,  0.6742, -0.1535,  1.6858, -0.9125, -1.9437],\n         [ 1.0000,  0.6742, -0.1535,  1.6864, -0.9000, -1.8218],\n         [ 1.0000,  0.6742, -0.1535,  1.6871, -0.8875, -1.8218],\n         [ 1.0000,  0.6742, -0.1535,  1.6877, -0.8750, -1.8044],\n         [ 1.0000,  0.6742, -0.1535,  1.6883, -0.8625, -1.1951],\n         [ 1.0000,  0.6742, -0.1535,  1.6890, -0.8500, -0.8469],\n         [ 1.0000,  0.6742, -0.1535,  1.6896, -0.8375, -0.6728],\n         [ 1.0000,  0.6742, -0.1535,  1.6902, -0.8250, -0.6162],\n         [ 1.0000,  0.6742, -0.1535,  1.6909, -0.8125, -0.7163],\n         [ 1.0000,  0.6742, -0.1535,  1.6915, -0.8000, -0.4203],\n         [ 1.0000,  0.6742, -0.1535,  1.6922, -0.7875, -0.5683],\n         [ 1.0000,  0.6742, -0.1535,  1.6928, -0.7750, -0.4203],\n         [ 1.0000,  0.6742, -0.1535,  1.6934, -0.7625, -0.6336],\n         [ 1.0000,  0.6742, -0.1535,  1.6941, -0.7500, -0.5727],\n         [ 1.0000,  0.6742, -0.1535,  1.6947, -0.7375, -0.5727],\n         [ 1.0000,  0.6742, -0.1535,  1.6953, -0.7250, -0.5509],\n         [ 1.0000,  0.6742, -0.1535,  1.6960, -0.7125, -0.3899],\n         [ 1.0000,  0.6742, -0.1535,  1.6966, -0.7000, -0.5727],\n         [ 1.0000,  0.6742, -0.1535,  1.6973, -0.6875, -0.4203],\n         [ 1.0000,  0.6742, -0.1535,  1.6979, -0.6750, -0.3159],\n         [ 1.0000,  0.6742, -0.1535,  1.6985, -0.6625, -0.4334],\n         [ 1.0000,  0.6742, -0.1535,  1.6992, -0.6500, -0.3768],\n         [ 1.0000,  0.6742, -0.1535,  1.6998, -0.6375, -0.3594],\n         [ 1.0000,  0.6742, -0.1535,  1.7005, -0.6250, -0.2985],\n         [ 1.0000,  0.6742, -0.1535,  1.7011, -0.6125, -0.2375],\n         [ 1.0000,  0.6742, -0.1535,  1.7017, -0.6000, -0.3072],\n         [ 1.0000,  0.6742, -0.1535,  1.7024, -0.5875, -0.4029],\n         [ 1.0000,  0.6742, -0.1535,  1.7030, -0.5750, -0.2811],\n         [ 1.0000,  0.6742, -0.1535,  1.7036, -0.5625, -0.1766],\n         [ 1.0000,  0.6742, -0.1535,  1.7043, -0.5500, -0.2027],\n         [ 1.0000,  0.6742, -0.1535,  1.7049, -0.5375,  0.1194],\n         [ 1.0000,  0.6742, -0.1535,  1.7056, -0.5250,  0.1020],\n         [ 1.0000,  0.6742, -0.1535,  1.7062, -0.5125,  0.2238],\n         [ 1.0000,  0.6742, -0.1535,  1.7068, -0.5000,  0.0410],\n         [ 1.0000,  0.6742, -0.1535,  1.7075, -0.4875,  0.4458],\n         [ 1.0000,  0.6742, -0.1535,  1.7081, -0.4750,  0.2673],\n         [ 1.0000,  0.6742, -0.1535,  1.7088, -0.4625,  0.5807],\n         [ 1.0000,  0.6742, -0.1535,  1.7094, -0.4500,  0.3283],\n         [ 1.0000,  0.6742, -0.1535,  1.7100, -0.4375,  0.4632],\n         [ 1.0000,  0.6742, -0.1535,  1.7107, -0.4250,  0.3283],\n         [ 1.0000,  0.6742, -0.1535,  1.7113, -0.4125,  0.7287],\n         [ 1.0000,  0.6742, -0.1535,  1.7119, -0.4000,  0.5416],\n         [ 1.0000,  0.6742, -0.1535,  1.7126, -0.3875,  0.3283],\n         [ 1.0000,  0.6742, -0.1535,  1.7132, -0.3750,  0.5720],\n         [ 1.0000,  0.6742, -0.1535,  1.7139, -0.3625,  0.6547],\n         [ 1.0000,  0.6742, -0.1535,  1.7145, -0.3500,  0.7069],\n         [ 1.0000,  0.6742, -0.1535,  1.7151, -0.3375,  0.8201],\n         [ 1.0000,  0.6742, -0.1535,  1.7158, -0.3250,  0.9812],\n         [ 1.0000,  0.6742, -0.1535,  1.7164, -0.3125,  0.7374],\n         [ 1.0000,  0.6742, -0.1535,  1.7171, -0.3000,  0.8941],\n         [ 1.0000,  0.6742, -0.1535,  1.7177, -0.2875,  1.1030],\n         [ 1.0000,  0.6742, -0.1535,  1.7183, -0.2750,  0.8985],\n         [ 1.0000,  0.6742, -0.1535,  1.7190, -0.2625,  0.9202],\n         [ 1.0000,  0.6742, -0.1535,  1.7196, -0.2500,  0.8506],\n         [ 1.0000,  0.6742, -0.1535,  1.7202, -0.2375,  0.9724],\n         [ 1.0000,  0.6742, -0.1535,  1.7209, -0.2250,  0.9028],\n         [ 1.0000,  0.6742, -0.1535,  1.7215, -0.2125,  0.9289],\n         [ 1.0000,  0.6742, -0.1535,  1.7222, -0.2000,  1.0247],\n         [ 1.0000,  0.6742, -0.1535,  1.7228, -0.1875,  1.0856],\n         [ 1.0000,  0.6742, -0.1535,  1.7234, -0.1750,  1.0987],\n         [ 1.0000,  0.6742, -0.1535,  1.7241, -0.1625,  1.1117],\n         [ 1.0000,  0.6742, -0.1535,  1.7247, -0.1500,  1.4208],\n         [ 1.0000,  0.6742, -0.1535,  1.7253, -0.1375,  1.1901],\n         [ 1.0000,  0.6742, -0.1535,  1.7260, -0.1250,  1.2945],\n         [ 1.0000,  0.6742, -0.1535,  1.7266, -0.1125,  1.3903],\n         [ 1.0000,  0.6742, -0.1535,  1.7273, -0.1000,  1.1901],\n         [ 1.0000,  0.6742, -0.1535,  1.7279, -0.0875,  0.9202],\n         [ 1.0000,  0.6742, -0.1535,  1.7285, -0.0750,  0.7418],\n         [ 1.0000,  0.6742, -0.1535,  1.7292, -0.0625,  0.7983],\n         [ 1.0000,  0.6742, -0.1535,  1.7298, -0.0500,  0.8593],\n         [ 1.0000,  0.6742, -0.1535,  1.7305, -0.0375,  1.0769],\n         [ 1.0000,  0.6742, -0.1535,  1.7311, -0.0250,  0.9463],\n         [ 1.0000,  0.6742, -0.1535,  1.7317, -0.0125,  0.9899]]],\n       device='cuda:0'), 'encoder_target': tensor([[1.7132, 1.5441, 1.5294, 1.8382, 1.5809, 2.0074, 1.6912, 1.7059, 1.9118,\n         1.9118, 1.9412, 2.9706, 3.5588, 3.8529, 3.9485, 3.7794, 4.2794, 4.0294,\n         4.2794, 3.9191, 4.0221, 4.0221, 4.0588, 4.3309, 4.0221, 4.2794, 4.4559,\n         4.2574, 4.3529, 4.3824, 4.4853, 4.5882, 4.4706, 4.3088, 4.5147, 4.6912,\n         4.6471, 5.1912, 5.1618, 5.3676, 5.0588, 5.7426, 5.4412, 5.9706, 5.5441,\n         5.7721, 5.5441, 6.2206, 5.9044, 5.5441, 5.9559, 6.0956, 6.1838, 6.3750,\n         6.6471, 6.2353, 6.5000, 6.8529, 6.5074, 6.5441, 6.4265, 6.6324, 6.5147,\n         6.5588, 6.7206, 6.8235, 6.8456, 6.8676, 7.3897, 7.0000, 7.1765, 7.3382,\n         7.0000, 6.5441, 6.2426, 6.3382, 6.4412, 6.8088, 6.5882, 6.6618]],\n       device='cuda:0'), 'encoder_lengths': tensor([80], device='cuda:0'), 'decoder_cat': tensor([], device='cuda:0', size=(1, 20, 0), dtype=torch.int64), 'decoder_cont': tensor([[[ 1.0000,  0.6742, -0.1535,  1.7324,  0.0000,  1.2379],\n         [ 1.0000,  0.6742, -0.1535,  1.7330,  0.0125,  0.9594],\n         [ 1.0000,  0.6742, -0.1535,  1.7336,  0.0250,  1.0247],\n         [ 1.0000,  0.6742, -0.1535,  1.7343,  0.0375,  1.0464],\n         [ 1.0000,  0.6742, -0.1535,  1.7349,  0.0500,  1.0987],\n         [ 1.0000,  0.6742, -0.1535,  1.7356,  0.0625,  1.2118],\n         [ 1.0000,  0.6742, -0.1535,  1.7362,  0.0750,  1.0595],\n         [ 1.0000,  0.6742, -0.1535,  1.7368,  0.0875,  1.2902],\n         [ 1.0000,  0.6742, -0.1535,  1.7375,  0.1000,  1.1161],\n         [ 1.0000,  0.6742, -0.1535,  1.7381,  0.1125,  1.2423],\n         [ 1.0000,  0.6742, -0.1535,  1.7388,  0.1250,  1.3337],\n         [ 1.0000,  0.6742, -0.1535,  1.7394,  0.1375,  1.2641],\n         [ 1.0000,  0.6742, -0.1535,  1.7400,  0.1500,  1.2597],\n         [ 1.0000,  0.6742, -0.1535,  1.7407,  0.1625,  1.3032],\n         [ 1.0000,  0.6742, -0.1535,  1.7413,  0.1750,  1.2205],\n         [ 1.0000,  0.6742, -0.1535,  1.7419,  0.1875, -1.4388],\n         [ 1.0000,  0.6742, -0.1535,  1.7426,  0.2000, -2.0220],\n         [ 1.0000,  0.6742, -0.1535,  1.7432,  0.2125, -1.9655],\n         [ 1.0000,  0.6742, -0.1535,  1.7439,  0.2250, -2.0830],\n         [ 1.0000,  0.6742, -0.1535,  1.7445,  0.2375, -1.9872]]],\n       device='cuda:0'), 'decoder_target': tensor([[7.0809, 6.6103, 6.7206, 6.7574, 6.8456, 7.0368, 6.7794, 7.1691, 6.8750,\n         7.0882, 7.2426, 7.1250, 7.1176, 7.1912, 7.0515, 2.5588, 1.5735, 1.6691,\n         1.4706, 1.6324]], device='cuda:0'), 'decoder_lengths': tensor([20], device='cuda:0'), 'decoder_time_idx': tensor([[5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438,\n         5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446]], device='cuda:0'), 'groups': tensor([[0]], device='cuda:0'), 'target_scale': tensor([[4.9895, 1.6894]], device='cuda:0')}, index=None, decoder_lengths=None, y=None)"}
