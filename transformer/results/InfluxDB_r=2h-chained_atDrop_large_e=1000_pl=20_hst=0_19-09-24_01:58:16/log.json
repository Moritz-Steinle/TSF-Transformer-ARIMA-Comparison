{"label":"InfluxDB_r=2h-chained_atDrop_large_e=1000_pl=20_hst=0","mean_squared_error":-1,"runtimes":"Training: 216.61 seconds","length_train_dataset":49022,"length_test_dataset":1,"parameters":"Epochs: 1000, hyperparameters: Hyperparamters(gradient_clip_val=6.9953515571, hidden_continuous_size=40, dropout=0.1558743686, attention_head_size=3, learning_rate=0.0039810717, accelerator='auto', hidden_size=70)","prediction":"Prediction(output=Output(prediction=tensor([[[2.6043, 5.5086, 6.6623, 7.0594, 7.3734, 7.7081, 7.7800],\n         [2.5202, 5.3027, 6.5875, 7.0570, 7.3907, 7.6937, 7.8056],\n         [2.3362, 5.0380, 6.4319, 7.0060, 7.3844, 7.6782, 7.8527],\n         [2.0481, 4.6855, 6.1945, 6.9175, 7.3724, 7.6732, 7.9265],\n         [1.6021, 4.1730, 5.8276, 6.7676, 7.3507, 7.6826, 8.0333],\n         [0.9199, 3.4002, 5.2557, 6.5064, 7.3053, 7.7148, 8.1779],\n         [0.0000, 2.2904, 4.4124, 6.0642, 7.2123, 7.7916, 8.3437],\n         [0.0000, 0.9673, 3.3781, 5.4179, 7.0535, 7.9311, 8.4575],\n         [0.0000, 0.0000, 2.4573, 4.7126, 6.8455, 8.0688, 8.4341],\n         [0.0000, 0.0000, 1.8510, 4.1465, 6.6441, 8.1265, 8.3141],\n         [0.0000, 0.0000, 1.5088, 3.7647, 6.4820, 8.1110, 8.1797],\n         [0.0000, 0.0000, 1.3139, 3.5094, 6.3551, 8.0616, 8.0639],\n         [0.0000, 0.0000, 1.1930, 3.3269, 6.2504, 7.9984, 7.9665],\n         [0.0000, 0.0000, 1.1100, 3.1865, 6.1583, 7.9288, 7.8813],\n         [0.0000, 0.0000, 1.0484, 3.0727, 6.0745, 7.8568, 7.8045],\n         [0.0000, 0.0000, 0.9998, 2.9777, 5.9976, 7.7852, 7.7344],\n         [0.0000, 0.0000, 0.9596, 2.8970, 5.9271, 7.7164, 7.6706],\n         [0.0000, 0.0000, 0.9246, 2.8275, 5.8627, 7.6516, 7.6126],\n         [0.0000, 0.0000, 0.8925, 2.7667, 5.8034, 7.5907, 7.5597],\n         [0.0000, 0.0000, 0.8612, 2.7123, 5.7476, 7.5325, 7.5105]]],\n       device='cuda:0'), encoder_attention=tensor([[[[0.0032, 0.0070, 0.0210,  ..., 0.0041, 0.0040, 0.0041],\n          [0.0247, 0.0378, 0.0642,  ..., 0.0059, 0.0059, 0.0060],\n          [0.0130, 0.0253, 0.0505,  ..., 0.0097, 0.0100, 0.0101],\n          [0.0094, 0.0136, 0.0179,  ..., 0.0121, 0.0127, 0.0125]],\n\n         [[0.0029, 0.0064, 0.0189,  ..., 0.0037, 0.0036, 0.0037],\n          [0.0224, 0.0339, 0.0572,  ..., 0.0064, 0.0063, 0.0064],\n          [0.0127, 0.0245, 0.0484,  ..., 0.0096, 0.0099, 0.0100],\n          [0.0092, 0.0132, 0.0174,  ..., 0.0122, 0.0127, 0.0126]],\n\n         [[0.0026, 0.0058, 0.0172,  ..., 0.0033, 0.0033, 0.0033],\n          [0.0203, 0.0303, 0.0509,  ..., 0.0065, 0.0064, 0.0065],\n          [0.0123, 0.0236, 0.0463,  ..., 0.0095, 0.0098, 0.0099],\n          [0.0090, 0.0128, 0.0167,  ..., 0.0124, 0.0129, 0.0128]],\n\n         ...,\n\n         [[0.0003, 0.0007, 0.0027,  ..., 0.0003, 0.0003, 0.0003],\n          [0.0027, 0.0034, 0.0049,  ..., 0.0044, 0.0043, 0.0044],\n          [0.0036, 0.0059, 0.0108,  ..., 0.0055, 0.0056, 0.0056],\n          [0.0049, 0.0054, 0.0057,  ..., 0.0140, 0.0143, 0.0141]],\n\n         [[0.0002, 0.0007, 0.0024,  ..., 0.0003, 0.0003, 0.0003],\n          [0.0026, 0.0032, 0.0047,  ..., 0.0041, 0.0040, 0.0041],\n          [0.0032, 0.0054, 0.0099,  ..., 0.0051, 0.0051, 0.0052],\n          [0.0048, 0.0052, 0.0055,  ..., 0.0138, 0.0141, 0.0139]],\n\n         [[0.0002, 0.0006, 0.0022,  ..., 0.0003, 0.0003, 0.0003],\n          [0.0025, 0.0031, 0.0046,  ..., 0.0038, 0.0037, 0.0038],\n          [0.0029, 0.0049, 0.0091,  ..., 0.0047, 0.0047, 0.0048],\n          [0.0047, 0.0050, 0.0054,  ..., 0.0136, 0.0139, 0.0137]]]],\n       device='cuda:0'), decoder_attention=tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.1055, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0757, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0301, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0063, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0953, 0.0895, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0768, 0.0766, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0300, 0.0337, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0065, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         ...,\n\n         [[0.0141, 0.0134, 0.0139,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0368, 0.0363, 0.0368,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0214, 0.0242, 0.0261,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0111, 0.0121, 0.0127,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0125, 0.0118, 0.0122,  ..., 0.1070, 0.0000, 0.0000],\n          [0.0345, 0.0339, 0.0344,  ..., 0.0629, 0.0000, 0.0000],\n          [0.0203, 0.0230, 0.0249,  ..., 0.0544, 0.0000, 0.0000],\n          [0.0110, 0.0120, 0.0126,  ..., 0.0145, 0.0000, 0.0000]],\n\n         [[0.0110, 0.0105, 0.0108,  ..., 0.0963, 0.1021, 0.0000],\n          [0.0323, 0.0318, 0.0323,  ..., 0.0590, 0.0612, 0.0000],\n          [0.0193, 0.0219, 0.0237,  ..., 0.0524, 0.0517, 0.0000],\n          [0.0108, 0.0118, 0.0124,  ..., 0.0144, 0.0143, 0.0000]]]],\n       device='cuda:0'), static_variables=tensor([[[0.7957, 0.1099, 0.0944]]], device='cuda:0'), encoder_variables=tensor([[[[0.0480, 0.2708, 0.6812]],\n\n         [[0.0480, 0.2708, 0.6811]],\n\n         [[0.0480, 0.2707, 0.6813]],\n\n         [[0.0480, 0.2702, 0.6818]],\n\n         [[0.0480, 0.2703, 0.6816]],\n\n         [[0.0480, 0.2697, 0.6823]],\n\n         [[0.0480, 0.2699, 0.6821]],\n\n         [[0.0480, 0.2697, 0.6822]],\n\n         [[0.0481, 0.2693, 0.6826]],\n\n         [[0.0481, 0.2692, 0.6828]],\n\n         [[0.0481, 0.2690, 0.6830]],\n\n         [[0.0481, 0.2674, 0.6845]],\n\n         [[0.0481, 0.2662, 0.6857]],\n\n         [[0.0481, 0.2654, 0.6864]],\n\n         [[0.0482, 0.2651, 0.6868]],\n\n         [[0.0482, 0.2653, 0.6866]],\n\n         [[0.0482, 0.2640, 0.6878]],\n\n         [[0.0482, 0.2644, 0.6874]],\n\n         [[0.0482, 0.2637, 0.6881]],\n\n         [[0.0482, 0.2644, 0.6875]],\n\n         [[0.0482, 0.2640, 0.6878]],\n\n         [[0.0482, 0.2638, 0.6880]],\n\n         [[0.0482, 0.2636, 0.6882]],\n\n         [[0.0482, 0.2628, 0.6890]],\n\n         [[0.0482, 0.2634, 0.6884]],\n\n         [[0.0482, 0.2626, 0.6892]],\n\n         [[0.0482, 0.2620, 0.6897]],\n\n         [[0.0482, 0.2624, 0.6894]],\n\n         [[0.0482, 0.2620, 0.6898]],\n\n         [[0.0482, 0.2617, 0.6900]],\n\n         [[0.0482, 0.2613, 0.6904]],\n\n         [[0.0483, 0.2609, 0.6908]],\n\n         [[0.0483, 0.2611, 0.6907]],\n\n         [[0.0482, 0.2613, 0.6904]],\n\n         [[0.0483, 0.2606, 0.6911]],\n\n         [[0.0483, 0.2600, 0.6917]],\n\n         [[0.0483, 0.2600, 0.6917]],\n\n         [[0.0483, 0.2582, 0.6934]],\n\n         [[0.0483, 0.2582, 0.6935]],\n\n         [[0.0484, 0.2574, 0.6943]],\n\n         [[0.0483, 0.2582, 0.6935]],\n\n         [[0.0484, 0.2557, 0.6959]],\n\n         [[0.0484, 0.2566, 0.6950]],\n\n         [[0.0484, 0.2545, 0.6971]],\n\n         [[0.0484, 0.2560, 0.6956]],\n\n         [[0.0484, 0.2550, 0.6966]],\n\n         [[0.0484, 0.2557, 0.6959]],\n\n         [[0.0485, 0.2528, 0.6987]],\n\n         [[0.0485, 0.2540, 0.6975]],\n\n         [[0.0484, 0.2552, 0.6964]],\n\n         [[0.0485, 0.2535, 0.6980]],\n\n         [[0.0485, 0.2528, 0.6988]],\n\n         [[0.0485, 0.2522, 0.6993]],\n\n         [[0.0485, 0.2512, 0.7002]],\n\n         [[0.0486, 0.2498, 0.7016]],\n\n         [[0.0485, 0.2515, 0.6999]],\n\n         [[0.0486, 0.2502, 0.7013]],\n\n         [[0.0486, 0.2483, 0.7031]],\n\n         [[0.0486, 0.2498, 0.7016]],\n\n         [[0.0486, 0.2495, 0.7019]],\n\n         [[0.0486, 0.2499, 0.7015]],\n\n         [[0.0486, 0.2488, 0.7026]],\n\n         [[0.0486, 0.2492, 0.7022]],\n\n         [[0.0486, 0.2488, 0.7026]],\n\n         [[0.0486, 0.2479, 0.7035]],\n\n         [[0.0487, 0.2472, 0.7042]],\n\n         [[0.0487, 0.2469, 0.7044]],\n\n         [[0.0487, 0.2466, 0.7047]],\n\n         [[0.0488, 0.2438, 0.7075]],\n\n         [[0.0487, 0.2456, 0.7057]],\n\n         [[0.0488, 0.2445, 0.7067]],\n\n         [[0.0488, 0.2436, 0.7076]],\n\n         [[0.0487, 0.2452, 0.7061]],\n\n         [[0.0487, 0.2473, 0.7040]],\n\n         [[0.0486, 0.2486, 0.7028]],\n\n         [[0.0486, 0.2480, 0.7034]],\n\n         [[0.0487, 0.2474, 0.7040]],\n\n         [[0.0487, 0.2454, 0.7059]],\n\n         [[0.0487, 0.2463, 0.7050]],\n\n         [[0.0487, 0.2458, 0.7055]]]], device='cuda:0'), decoder_variables=tensor([[[[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]],\n\n         [[0.1256, 0.8744]]]], device='cuda:0'), decoder_lengths=tensor([20], device='cuda:0'), encoder_lengths=tensor([80], device='cuda:0')), x={'encoder_cat': tensor([], device='cuda:0', size=(1, 80, 0), dtype=torch.int64), 'encoder_cont': tensor([[[ 1.0000,  0.6698, -0.1565,  1.7264, -1.0000, -1.9393],\n         [ 1.0000,  0.6698, -0.1565,  1.7265, -0.9875, -2.0395],\n         [ 1.0000,  0.6698, -0.1565,  1.7266, -0.9750, -2.0482],\n         [ 1.0000,  0.6698, -0.1565,  1.7266, -0.9625, -1.8654],\n         [ 1.0000,  0.6698, -0.1565,  1.7267, -0.9500, -2.0177],\n         [ 1.0000,  0.6698, -0.1565,  1.7268, -0.9375, -1.7652],\n         [ 1.0000,  0.6698, -0.1565,  1.7269, -0.9250, -1.9524],\n         [ 1.0000,  0.6698, -0.1565,  1.7269, -0.9125, -1.9437],\n         [ 1.0000,  0.6698, -0.1565,  1.7270, -0.9000, -1.8218],\n         [ 1.0000,  0.6698, -0.1565,  1.7271, -0.8875, -1.8218],\n         [ 1.0000,  0.6698, -0.1565,  1.7271, -0.8750, -1.8044],\n         [ 1.0000,  0.6698, -0.1565,  1.7272, -0.8625, -1.1951],\n         [ 1.0000,  0.6698, -0.1565,  1.7273, -0.8500, -0.8469],\n         [ 1.0000,  0.6698, -0.1565,  1.7273, -0.8375, -0.6728],\n         [ 1.0000,  0.6698, -0.1565,  1.7274, -0.8250, -0.6162],\n         [ 1.0000,  0.6698, -0.1565,  1.7275, -0.8125, -0.7163],\n         [ 1.0000,  0.6698, -0.1565,  1.7276, -0.8000, -0.4203],\n         [ 1.0000,  0.6698, -0.1565,  1.7276, -0.7875, -0.5683],\n         [ 1.0000,  0.6698, -0.1565,  1.7277, -0.7750, -0.4203],\n         [ 1.0000,  0.6698, -0.1565,  1.7278, -0.7625, -0.6336],\n         [ 1.0000,  0.6698, -0.1565,  1.7278, -0.7500, -0.5727],\n         [ 1.0000,  0.6698, -0.1565,  1.7279, -0.7375, -0.5727],\n         [ 1.0000,  0.6698, -0.1565,  1.7280, -0.7250, -0.5509],\n         [ 1.0000,  0.6698, -0.1565,  1.7281, -0.7125, -0.3899],\n         [ 1.0000,  0.6698, -0.1565,  1.7281, -0.7000, -0.5727],\n         [ 1.0000,  0.6698, -0.1565,  1.7282, -0.6875, -0.4203],\n         [ 1.0000,  0.6698, -0.1565,  1.7283, -0.6750, -0.3159],\n         [ 1.0000,  0.6698, -0.1565,  1.7283, -0.6625, -0.4334],\n         [ 1.0000,  0.6698, -0.1565,  1.7284, -0.6500, -0.3768],\n         [ 1.0000,  0.6698, -0.1565,  1.7285, -0.6375, -0.3594],\n         [ 1.0000,  0.6698, -0.1565,  1.7286, -0.6250, -0.2985],\n         [ 1.0000,  0.6698, -0.1565,  1.7286, -0.6125, -0.2375],\n         [ 1.0000,  0.6698, -0.1565,  1.7287, -0.6000, -0.3072],\n         [ 1.0000,  0.6698, -0.1565,  1.7288, -0.5875, -0.4029],\n         [ 1.0000,  0.6698, -0.1565,  1.7288, -0.5750, -0.2811],\n         [ 1.0000,  0.6698, -0.1565,  1.7289, -0.5625, -0.1766],\n         [ 1.0000,  0.6698, -0.1565,  1.7290, -0.5500, -0.2027],\n         [ 1.0000,  0.6698, -0.1565,  1.7290, -0.5375,  0.1194],\n         [ 1.0000,  0.6698, -0.1565,  1.7291, -0.5250,  0.1020],\n         [ 1.0000,  0.6698, -0.1565,  1.7292, -0.5125,  0.2238],\n         [ 1.0000,  0.6698, -0.1565,  1.7293, -0.5000,  0.0410],\n         [ 1.0000,  0.6698, -0.1565,  1.7293, -0.4875,  0.4458],\n         [ 1.0000,  0.6698, -0.1565,  1.7294, -0.4750,  0.2673],\n         [ 1.0000,  0.6698, -0.1565,  1.7295, -0.4625,  0.5807],\n         [ 1.0000,  0.6698, -0.1565,  1.7295, -0.4500,  0.3283],\n         [ 1.0000,  0.6698, -0.1565,  1.7296, -0.4375,  0.4632],\n         [ 1.0000,  0.6698, -0.1565,  1.7297, -0.4250,  0.3283],\n         [ 1.0000,  0.6698, -0.1565,  1.7298, -0.4125,  0.7287],\n         [ 1.0000,  0.6698, -0.1565,  1.7298, -0.4000,  0.5416],\n         [ 1.0000,  0.6698, -0.1565,  1.7299, -0.3875,  0.3283],\n         [ 1.0000,  0.6698, -0.1565,  1.7300, -0.3750,  0.5720],\n         [ 1.0000,  0.6698, -0.1565,  1.7300, -0.3625,  0.6547],\n         [ 1.0000,  0.6698, -0.1565,  1.7301, -0.3500,  0.7069],\n         [ 1.0000,  0.6698, -0.1565,  1.7302, -0.3375,  0.8201],\n         [ 1.0000,  0.6698, -0.1565,  1.7302, -0.3250,  0.9812],\n         [ 1.0000,  0.6698, -0.1565,  1.7303, -0.3125,  0.7374],\n         [ 1.0000,  0.6698, -0.1565,  1.7304, -0.3000,  0.8941],\n         [ 1.0000,  0.6698, -0.1565,  1.7305, -0.2875,  1.1030],\n         [ 1.0000,  0.6698, -0.1565,  1.7305, -0.2750,  0.8985],\n         [ 1.0000,  0.6698, -0.1565,  1.7306, -0.2625,  0.9202],\n         [ 1.0000,  0.6698, -0.1565,  1.7307, -0.2500,  0.8506],\n         [ 1.0000,  0.6698, -0.1565,  1.7307, -0.2375,  0.9724],\n         [ 1.0000,  0.6698, -0.1565,  1.7308, -0.2250,  0.9028],\n         [ 1.0000,  0.6698, -0.1565,  1.7309, -0.2125,  0.9289],\n         [ 1.0000,  0.6698, -0.1565,  1.7310, -0.2000,  1.0247],\n         [ 1.0000,  0.6698, -0.1565,  1.7310, -0.1875,  1.0856],\n         [ 1.0000,  0.6698, -0.1565,  1.7311, -0.1750,  1.0987],\n         [ 1.0000,  0.6698, -0.1565,  1.7312, -0.1625,  1.1117],\n         [ 1.0000,  0.6698, -0.1565,  1.7312, -0.1500,  1.4208],\n         [ 1.0000,  0.6698, -0.1565,  1.7313, -0.1375,  1.1901],\n         [ 1.0000,  0.6698, -0.1565,  1.7314, -0.1250,  1.2945],\n         [ 1.0000,  0.6698, -0.1565,  1.7314, -0.1125,  1.3903],\n         [ 1.0000,  0.6698, -0.1565,  1.7315, -0.1000,  1.1901],\n         [ 1.0000,  0.6698, -0.1565,  1.7316, -0.0875,  0.9202],\n         [ 1.0000,  0.6698, -0.1565,  1.7317, -0.0750,  0.7418],\n         [ 1.0000,  0.6698, -0.1565,  1.7317, -0.0625,  0.7983],\n         [ 1.0000,  0.6698, -0.1565,  1.7318, -0.0500,  0.8593],\n         [ 1.0000,  0.6698, -0.1565,  1.7319, -0.0375,  1.0769],\n         [ 1.0000,  0.6698, -0.1565,  1.7319, -0.0250,  0.9463],\n         [ 1.0000,  0.6698, -0.1565,  1.7320, -0.0125,  0.9899]]],\n       device='cuda:0'), 'encoder_target': tensor([[1.7132, 1.5441, 1.5294, 1.8382, 1.5809, 2.0074, 1.6912, 1.7059, 1.9118,\n         1.9118, 1.9412, 2.9706, 3.5588, 3.8529, 3.9485, 3.7794, 4.2794, 4.0294,\n         4.2794, 3.9191, 4.0221, 4.0221, 4.0588, 4.3309, 4.0221, 4.2794, 4.4559,\n         4.2574, 4.3529, 4.3824, 4.4853, 4.5882, 4.4706, 4.3088, 4.5147, 4.6912,\n         4.6471, 5.1912, 5.1618, 5.3676, 5.0588, 5.7426, 5.4412, 5.9706, 5.5441,\n         5.7721, 5.5441, 6.2206, 5.9044, 5.5441, 5.9559, 6.0956, 6.1838, 6.3750,\n         6.6471, 6.2353, 6.5000, 6.8529, 6.5074, 6.5441, 6.4265, 6.6324, 6.5147,\n         6.5588, 6.7206, 6.8235, 6.8456, 6.8676, 7.3897, 7.0000, 7.1765, 7.3382,\n         7.0000, 6.5441, 6.2426, 6.3382, 6.4412, 6.8088, 6.5882, 6.6618]],\n       device='cuda:0'), 'encoder_lengths': tensor([80], device='cuda:0'), 'decoder_cat': tensor([], device='cuda:0', size=(1, 20, 0), dtype=torch.int64), 'decoder_cont': tensor([[[ 1.0000,  0.6698, -0.1565,  1.7321,  0.0000,  1.2379],\n         [ 1.0000,  0.6698, -0.1565,  1.7322,  0.0125,  0.9594],\n         [ 1.0000,  0.6698, -0.1565,  1.7322,  0.0250,  1.0247],\n         [ 1.0000,  0.6698, -0.1565,  1.7323,  0.0375,  1.0464],\n         [ 1.0000,  0.6698, -0.1565,  1.7324,  0.0500,  1.0987],\n         [ 1.0000,  0.6698, -0.1565,  1.7324,  0.0625,  1.2118],\n         [ 1.0000,  0.6698, -0.1565,  1.7325,  0.0750,  1.0595],\n         [ 1.0000,  0.6698, -0.1565,  1.7326,  0.0875,  1.2902],\n         [ 1.0000,  0.6698, -0.1565,  1.7327,  0.1000,  1.1161],\n         [ 1.0000,  0.6698, -0.1565,  1.7327,  0.1125,  1.2423],\n         [ 1.0000,  0.6698, -0.1565,  1.7328,  0.1250,  1.3337],\n         [ 1.0000,  0.6698, -0.1565,  1.7329,  0.1375,  1.2641],\n         [ 1.0000,  0.6698, -0.1565,  1.7329,  0.1500,  1.2597],\n         [ 1.0000,  0.6698, -0.1565,  1.7330,  0.1625,  1.3032],\n         [ 1.0000,  0.6698, -0.1565,  1.7331,  0.1750,  1.2205],\n         [ 1.0000,  0.6698, -0.1565,  1.7331,  0.1875, -1.4388],\n         [ 1.0000,  0.6698, -0.1565,  1.7332,  0.2000, -2.0220],\n         [ 1.0000,  0.6698, -0.1565,  1.7333,  0.2125, -1.9655],\n         [ 1.0000,  0.6698, -0.1565,  1.7334,  0.2250, -2.0830],\n         [ 1.0000,  0.6698, -0.1565,  1.7334,  0.2375, -1.9872]]],\n       device='cuda:0'), 'decoder_target': tensor([[7.0809, 6.6103, 6.7206, 6.7574, 6.8456, 7.0368, 6.7794, 7.1691, 6.8750,\n         7.0882, 7.2426, 7.1250, 7.1176, 7.1912, 7.0515, 2.5588, 1.5735, 1.6691,\n         1.4706, 1.6324]], device='cuda:0'), 'decoder_lengths': tensor([20], device='cuda:0'), 'decoder_time_idx': tensor([[49003, 49004, 49005, 49006, 49007, 49008, 49009, 49010, 49011, 49012,\n         49013, 49014, 49015, 49016, 49017, 49018, 49019, 49020, 49021, 49022]],\n       device='cuda:0'), 'groups': tensor([[0]], device='cuda:0'), 'target_scale': tensor([[4.9895, 1.6894]], device='cuda:0')}, index=None, decoder_lengths=None, y=None)"}
